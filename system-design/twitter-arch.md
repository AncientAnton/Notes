# Twitter使用的架构，处理 1.5 亿活跃用户，30 万 QPS，22 MB/S 的防火墙，5 秒内发推

解决Twitter"问题"的玩具解决方案是最受欢迎的可伸缩性。每个人都这样认为，推特是非常简单的。只需要轻轻一挥手，我们就拥有了一个可扩展的推特，就是这么简单。然后这并不像Twitter架构副总裁Raffi Krikorian在[Timelines at Scale](http://www.infoq.com/presentations/Twitter-时间线-Scalability)中描述的那样简单。如果你想了解Twitter是如何工作的，继续往下面读。

不知不觉中，Twitter已经成长起来了。从奋力挣扎的[three-tierish Ruby on Rails](http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster)网站变成了互联网的核心服务，我们经常去Twitter上看看其它服务是不是挂了。

Twitter现在在全世界有 1.5 亿活跃用户，以每秒 30 万请求的处理速度生成时间线，每秒产生 22 MB流量的防火墙。系统每天处理 4 亿条推文，一条推文从Lady Gaga按下手指发布的瞬间，只需5分钟，就能推送给她的3100万关注者。

以下是值得关注的点：

- Twitter不仅仅只想是一个网站应用。Twitter希望成为一组驱动全世界移动客户端的服务，成为地球上最大的实时事件总线之一。

- Twitter主要机制是消费，而不是生产。每秒 30 万请求用于阅读时间线，而每秒仅 6000 个请求用于写入。

- 拥有大量关注者的人越来越常见。发送一个来自拥有大量关注者（大规模的推送）的用户的推文可能会很慢。Twitter试图在5秒内完成，但它并不总是起作用，特别是当名人相互转推时，这种情况越来越多。其后果之一是回复会在收到原始推文之前到达。Twitter的工作，在从如何写入推文，变为如何从高价值用户读取推文。

- 您的主页时间线位于 Redis 集群中，最多包含 800 个条目。

- Twitter从你关注的人以及你点击的链接中了解到很多关于你的事情。当双向关注关系不存在时，可以根据隐性社交关系来推断。

- 用户关心推文，但推文的文本与Twitter的大部分基础设施几乎无关。

- 需要一个非常精良的监视和调试系统来跟踪复杂堆栈中的性能问题。历史遗留的决策问题总是困扰着这个系统。

所以Twitter是如何工作的？

## 挑战

- 在 1.5 亿用户与 每秒 30 万的时间线请求（主页和搜索），原始的处理可能很慢。
- 原始的处理是在所有的推文上做一个巨大的SELECT，它被审判和死亡。
- 解决方案是基于写入的推送方法。当推文到达时，执行大量处理，以找出推文应该推送到哪里。这使得读取时间线变得快速和容易。不要在读取时进行任何计算。在写入上执行的所有工作，处理速度都比读取慢，速度为每秒 4000 个请求。

## 团队

*平台服务团队负责Twitter的核心可扩展基础架构*

- 他们负责时间线服务，推文服务，用户服务，社交关系图服务，以及所有支持Twitter平台的服务。
- 内部客户端使用与外部客户端大致相同的 API。
- 100 万个应用注册使用Twitter的第三方 API
- 与产品团队约定不必担心规模。
- 规划处理容量，构建可扩展的后端系统，随着站点以意想不到的方式增长，不断的更换基础架构。

## 推/拉

* 人们一直在Twitter上创建内容。推特的工作是找出如何联系内容。如何将其发送给您的关注者。

* 真正的挑战是实时性。目标是在不超过 5 秒内向用户交付消息流。
    * 交付意味着收集内容，并在互联网上施加压力，使其尽快恢复。
    * 交付到内存中的时间线集群，推送通知，触发的电子邮件，所有 iOS 通知以及黑莓和安卓，SMS短信。
    * Twitter 是世界上每个活跃用户中最大的 SMS 生成器。
    * 选举可能是内容进入和内容外出的最大驱动力之一。

* 两种主要类型的时间线：用户时间线和主页时间线。
    * 用户时间线是特定用户发送的所有推文。
    * 主页时间线是您所关注人的所有用户时间线的合并。
    * 具有业务规则。未关注人的回复会被剔除，指定用户的转推也可以被剔除。
    * 在Twitter上做这个是非常具有挑战性的。

* 基于拉

    * 特定时间线。诸如twitter.com和主页时间线之类的内容。在请求时将推文发送给您。
    * 查询时间线。通过搜索 API查询。尽可能快地返回与特定查询匹配的所有推文。

* 基于推
    * Twitter 运行着最大的实时事件系统之一，以 22 MB/秒的速度推送推文。
        * 与Twitter建立一个Socket连接，在150毫秒内推送所有公开的推文。
        * 在任何时间，大约有 100 万个Socket连接到推送集群。
        * 搜索引擎会从这些Socket中获取所以推文。
        * 上述想法是无法实现的
    * 用户流连接。支持 TweetDeck 和 Twitter For Mac。当您登录时，他们会查看您的社交关系图，并且仅从您关注的人收取消息，来重新创建主页时间线。通过持久连接而不是轮询，一样可以获得相同的时间线体验。
    * 查询 API。针对推文发起长期查询。当创建推文并发现与查询匹配时，它们被路由到该查询的已注册的Socket连接中。

## 高级拉取时间线

* 推文通过写入 API 提供。它通过负载均衡器和 TFE（Twitter 前端）和其它模块。
* 这是一条非常直接的路径，完全预先计算好主页时间线。所有业务规则都随着推文的进入而执行。
* 扇出过程是立即生效的。进来的推文被放入一个庞大的 Redis 集群中。每个推文在 3 台不同的计算机上复制 3 次。在Twitter这个规模上，一天内会有许多机器失败。
* 扇出进程会查询基于 [Flock](https://github.com/twitter/flockdb) 的社交关系图服务。Flock 维护关注者和关注者列表。
    * Flock 返回对应的社交关系图，并开始遍历存储在 Redis 集群中的所有时间线。
    * Redis 集群具有几 TB 的容量。
    * 流水线一次推送推文到 4000 个时间线
    * 在 Redis 中使用原生数据结构
    * 假设你发推文，你有 2 万个关注者。扇出进程将找到 Redis 集群内所有 2 万个用户的位置。然后，它将开始在整个 Redis 集群的所有这些列表中插入推文的推文 ID。因此，每次编写推文时，Redis 集群中会发生多达 2万次 的插入。
    * 将存储的是生成的推文的推文 ID、推文发起者的用户 ID 以及用于标记该推文是转推、回复、其他类型的 4 个字节的位。
    * 主页时间线位于 Redis 集群中，长度为 800。内存容量是限制了时间线的长度。
    * 每个活跃用户的信息都存储在内存中，以降低延迟。
    * 活跃用户是在 30 天内登录 Twitter 的用户，根据缓存容量或 Twitter 的使用情况，这个数值可能可能会发生变化。
    * 如果您不是活跃用户，则推文不会进入缓存。
    * 只有主页时间线会命中磁盘。
    * 如果您从 Redis 集群中退出，则要经历一个称为重建的过程。
        * 查询社交关系图服务进行查询。找出你关注谁。查询磁盘，然后推回到Redis。
        * 通过Gizzard处理MySQL磁盘存储，它抽象了SQL事务并提供全局复制。
    * 如果机器出现问题，则复制 3 次，则不必为每个数据中心重新创建该计算机上所有时间线的时间线。
    * 如果推文实际上是转推，则指针将存储到原始推文。
* 当您查询您的主页时间线时，将查询时间线服务。然后，时间线服务只需找到一台具有主时间线的主机。
    * 有 3 个不同的哈希环在同时运行，因为您的时间线在 3 个不同的位置。
    * 只需要找到第一个命中的，并尽快返回。
    * 扇出需要更长的时间，但读取过程是快速的。从未预热的缓存到浏览器大约 2 秒。对于 API 调用，大约 400 毫秒。
* 由于时间线仅包含推文ID，他们必须"补全"这些推文，即查找推文的文本。给定一系列的 ID，执行获取多个的操作，并从 T-bird 并行获取推文。
* Gizmoduck 是用户服务，Tweetypie 是推文对象服务。每个服务都有自己的缓存。用户缓存是一个 mmcache 集群，其缓存中具有整个用户群。Tweetypie 有大约上个月和一半的推文存储在其 memcache 集群。这些客户向内部客户公开。
* 一些内容会在读取的时间过滤掉。例如，在法国过滤掉纳粹内容，因此在内容发送给用户之前，会在读取时进行过滤。

## 高级搜索

* 与拉取相反。所有计算操作都发生在读取时，写入变得简单。
* 当一条推文发送时，Ingester 会进行分词并找出他们想要索引的所有内容，并将其放入一台"Early Bird"机器中。"Early Bird"是Lucene的修改版本。索引存储在内存中。
* 在扇出过程中，推文可能存储在N个关注了你的人的主页时间线中。在"Early Bird"中，推文仅存储在一台"Early Bird"机器中（复制除外）。
* Blender用于创建搜索时间线。它必须将请求分散到数据中心。它查询每个"Early Bird"分片，并匹配此查询的内容。如果要求查询"纽约时报"，则查询所有分片，将返回结果排序、合并和重新排列。重新排名是通过社会影响力，比如转推数、收藏和回复的数量。
* 活动的信息是在写入时计算的，有一个单独的活动时间线。在您收藏和回复推文时，类似于主页时间线，它是一系列活动的 ID，因此有收藏 ID、回复 ID 等。
* 所有这些都被送入Blender。在读取路径上，它会重新计算、合并和排序。返回您视为搜索时间线的内容。
* 发现页是基于你的信息构建的自定义搜索。Twitter获取了很多信息，包括你关注的人，点击的链接等。它根据收集到的关于您的信息重新排名。

## 搜索和拉取是相对的

* 搜索和拉取看起来非常相似，但它们目的是相反的。

* 在主页时间线上：

    * 写。当推文发布时，有一个 O(n)花费的进程要写入 Redis 集群，其中 n 是关注您的人数。对LadyGaga和Barack Obama这种用户来说，花费非常大，需要数千万次的跨集群插入。所有 Redis 集群都会备份到磁盘，Flock 集群将用户时间线存储到磁盘，但通常时间线在 Redis 集群的内存中找到。
    * 读。通过 API 或 Web，有0(1)花费来找到合适的 Redis 机器。Twitter 经过优化，在主页时间线上的读取实现了高可用。读取花费在 10 毫秒内。推特主要是一种消费机制，而不是一种生产机制。每秒 30 万请求用于读取，6000 RPS 用于写入。
* 在搜索时间线上：

    * 写。当一条推文进来并命中Ingester时，只有一台"Early Bird"机器被击中。写入时间花费为0(1)。在排队和处理之间不到 5 秒内，会找到一台"Early Bird"将其写入。
    * 读。当读取进来时，它必须在整个群集中执行O(n)花费 读取。大多数人不使用搜索，因此他们可以有效地存储搜索推文。但需要付出即时性的代价。读取为 100 毫秒。搜索从不命中磁盘。整个 Lucene 索引在内存中中，因此读取效率很高，因为它们永远不会命中磁盘。

* 推文的文本与大多数基础结构几乎无关。T-bird 存储整个推文库。推文的大部分文本都位于内存中。如果未在内存中，则T-bird做一个查询，并存储到内存中。除了搜索、趋势或当下流行时间线外，文本几乎无关紧要。主页时间线也并不关注。

## 将来

* 如何使流水线更快、更高效？
* 扇出可能很慢。尝试在 5 秒内完成，但有时不起作用。很难，特别是当名人发微博时，这种情况越来越多。
* 推特的社交关系图是一个不对称的图形。推文仅呈现给在给定时间关注的用户。Twitter获取了很多你的信息，比如你可以关注Lance Armstrong，但他未关注你。当双向跟随不存在时，隐性社交关系可以代表很多信息。
* 基数很大是社交关系图的问题。@ladygaga有3100万追随者。@katyperry有2800万追随者。@justinbieber有2800万追随者。@barackobama有2300万追随者。
* 当其中一人发推文时，在数据中心中需要写入很多推文。当他们开始互相交谈时，这尤其具有挑战性，这种情况总是发生。
* 这些高扇出用户是Twitter面临的最大挑战。在名人的原始推文之前，一直可以看到回复。他们介绍整个场地的种族条件。如果从Lady Gaga发来的推文花几分钟时间，那么人们就会在不同的时间点看到她的推文。最近关注Lady Gaga的人可能会比过去跟踪她的人先5分钟看到她的推文。假设早期接收列表上的人员会回复该回复，然后正在处理该回复的扇出，而她的扇出仍在进行，因此在稍后收到她推文的用户的原始推文之前注入回复。导致大量用户混淆。推文在发布前按 ID 排序，因为它们大多是单调增加的，但这不能以这种规模解决问题。队列会一直备份，以便进行高值扇出。
* 尝试找出如何合并读取和写入路径。不再扇出高价值用户。对于像Taylor Swift这样的人，不再费心扇出，而是在阅读时合并在她的时间线中。平衡读取和写入路径。节省 10% 的计算资源。

## 解耦

* 推文以许多不同的方式被存储，主要是为了使团队彼此分离。搜索、推送、兴趣电子邮件和主页时间线团队可以相互独立工作。
* 出于性能原因，系统一直在解耦。推特过去是完全同步的。2 年前，由于性能原因，这一规定已经停止。将推文引入推文 API 最多需要 145 毫秒，然后所有客户端都断开连接。这是出于旧原因。写入路径由 Ruby 通过 MRI（单个线程服务器）供电，每次分配Unicorn 工作线程时，处理能力都会被消耗。他们希望能够尽可能快地释放客户端连接。有一条推文进来。Ruby 处理它。将其放入队列并断开连接。它们每个主机只运行大约 45-48 个进程，因此他们只能同时引入那么多推文，因此他们希望尽可能快地断开连接。
* 推文被传递到异步路径，我们一直在谈论的所有内容都踢开。

## 监控

* 办公室周围的仪表板显示系统在任意给定时间的性能。
* 如果你有100万关注者，它需要几秒钟来扇出所有的推文。
* 推文输入统计：每天4亿条推文;平均5K/秒;7K/秒每日峰值;大型活动期间的 ±12K/秒。
* 时间线交付统计：30b交付/天（+21m/min）;3.5 秒 = p50（第 50 百分位数），可交付至 1m;300k 交付/秒;• 99% 可能需要长达 5 分钟
* 名为 VIZ 的系统监视每个集群。从Scala群集获取数据的时间线服务的平均请求时间是5毫秒。99%是100ms以下。 其余情况下是命中磁盘，因此需要几百毫秒。
* Zipkin基于谷歌的Dapper系统。有了它，他们可以染色一个请求，并看到它命中的每一个服务，与请求时间，所以他们可以得到一个非常详细的性能数据。然后，可以向下并查看每个请求，并了解所有不同的计时。通过查看在请求上花费的时间，将花费大量时间来调试系统。还可以按阶段显示汇总统计信息，例如，查看扇出或交付的时间。这是一个2年的项目，以统计获取活跃用户时间线的时间的花费下降到2毫秒。花费了大量时间处理 GC 暂停、memcache 查找、了解数据中心的拓扑结构以及真正为此成功设置集群。

## 相关资料

* [Why Are Facebook, Digg, And Twitter So Hard To Scale?](http://highscalability.com/blog/2009/10/13/why-are-facebook-digg-and-twitter-so-hard-to-scale.html)
* [On Reddit](http://www.reddit.com/r/programming/comments/1hve87/the_architecture_twitter_uses_to_deal_with_150m/)
* [On Hacker News](https://news.ycombinator.com/item?id=6007650)
* [Real-Time Delivery Architecture at Twitter](http://www.infoq.com/presentations/Real-Time-Delivery-Twitter)
* [Paper: Feeding Frenzy: Selectively Materializing Users’ Event Feeds](http://highscalability.com/blog/2012/1/17/paper-feeding-frenzy-selectively-materializing-users-event-f.html)
* [Google: Taming The Long Latency Tail - When More Machines Equals Worse Results](http://highscalability.com/blog/2012/3/12/google-taming-the-long-latency-tail-when-more-machines-equal.html)
* [Did Facebook develop a custom in-memory database to manage its News Feeds?](http://www.quora.com/Facebook-Engineering/Did-Facebook-develop-a-custom-in-memory-database-to-manage-its-News-Feeds)(fan-out-on-read)